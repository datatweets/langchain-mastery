# Tutorial 2: Using Free AI Models with Hugging Face

## üéØ What You'll Learn

In this tutorial, you'll learn how to:

- Use AI models that run on your own computer (no internet needed!)
- Understand the difference between cloud AI and local AI
- Work with Hugging Face - the GitHub of AI models
- Generate text without paying for API calls

Think of this as having your own personal AI assistant that lives on your computer instead of the internet!

## üåü Why Use Local AI Models?

**Advantages:**
- **Free to use** - No API costs
- **Privacy** - Your data never leaves your computer
- **No internet required** - Works offline
- **No usage limits** - Generate as much text as you want

**Disadvantages:**
- **Slower** - Your computer does all the work
- **Less powerful** - Smaller models than GPT-4
- **Uses storage** - Models take up disk space

## üîç Understanding the Code: Line by Line

Let's examine `script_02.py` step by step:

### Step 1: Importing the AI Tools

```python
# Import the class for defining Hugging Face pipelines
from langchain_huggingface import HuggingFacePipeline
```

**What's happening here?**

1. **`from langchain_huggingface import HuggingFacePipeline`**
   - This imports a special tool for using Hugging Face AI models
   - Hugging Face is like a library of free AI models
   - `HuggingFacePipeline` is the tool that downloads and runs these models

**üí° What is Hugging Face?**
- It's like YouTube, but for AI models
- Thousands of people share their AI models there for free
- Anyone can download and use these models

### Step 2: Choosing and Loading an AI Model

```python
# Define the LLM from the Hugging Face model ID
llm = HuggingFacePipeline.from_model_id(
    # model_id="crumb/nano-mistral",
    model_id="microsoft/DialoGPT-medium",
    # model_id="gpt2",
    task="text-generation",
    pipeline_kwargs={"max_new_tokens": 20}
)
```

**What's happening here?**

1. **`llm = HuggingFacePipeline.from_model_id(...)`**
   - We're creating our AI assistant using a specific model
   - `from_model_id` means "download this specific AI model"

2. **`model_id="microsoft/DialoGPT-medium"`**
   - This is the "address" of the AI model we want
   - Format: `creator/model-name`
   - Microsoft created this model called DialoGPT-medium
   - It's designed for conversations (dialogue)

3. **`task="text-generation"`**
   - This tells the model what job to do
   - "text-generation" means "continue writing text"
   - Other tasks could be translation, summarization, etc.

4. **`pipeline_kwargs={"max_new_tokens": 20}`**
   - This controls how much text the AI generates
   - "max_new_tokens" = maximum number of words to add
   - 20 tokens ‚âà about 15-20 words

**ü§î What are those commented lines?**
```python
# model_id="crumb/nano-mistral",  # A very small, fast model
# model_id="gpt2",               # Classic model by OpenAI
```
These are other models you could try instead. They're "commented out" (disabled) with `#`.

### Step 3: Preparing Your Question

```python
prompt = "URL is"
```

**What's happening here?**

1. **`prompt = "URL is"`**
   - This is the beginning of a sentence we want the AI to complete
   - The AI will try to finish this sentence
   - It's like starting a story and asking the AI to continue

**üí° Why this specific prompt?**
- It's simple and short
- Most AI models can easily complete common phrases
- "URL is" might lead to responses like "URL is the address of a website"

### Step 4: Getting the AI Response

```python
# Invoke the model
response = llm.invoke(prompt)
print(response)
```

**What's happening here?**

1. **`response = llm.invoke(prompt)`**
   - We send our prompt to the AI model
   - The AI thinks about it and generates a response
   - The response gets stored in the `response` variable

2. **`print(response)`**
   - This displays the AI's response on your screen
   - You'll see both your original prompt and the AI's continuation

## üöÄ How to Run This Code

1. **Run the script**
   ```bash
   python script_02.py
   ```

2. **What happens during first run**
   - The computer downloads the AI model (this takes a few minutes)
   - The model gets stored on your computer for future use
   - The AI generates a response

3. **What you might see**
   ```
   URL is the address of a website on the internet that you can visit
   ```

## üß† What's Really Happening Behind the Scenes?

### First Time Running:
1. **Your computer** connects to Hugging Face's servers
2. **The AI model** (about 500MB-2GB) downloads to your computer
3. **Your computer** loads the model into memory
4. **The AI** processes your prompt using your computer's processing power
5. **The response** appears on your screen

### Subsequent Runs:
1. **Your computer** loads the already-downloaded model
2. **Processing happens locally** - much faster!
3. **No internet needed** - works completely offline

## üéì Key Concepts You've Learned

### Local vs. Cloud AI
- **Cloud AI (like OpenAI)**: Runs on company servers, needs internet, costs money
- **Local AI (like Hugging Face)**: Runs on your computer, works offline, free

### Model IDs
- **What**: Unique addresses for AI models on Hugging Face
- **Format**: `creator-name/model-name`
- **Examples**: `microsoft/DialoGPT-medium`, `gpt2`, `facebook/bart-large`

### Tokens
- **What**: Units that AI models use to measure text
- **Roughly**: 1 token ‚âà 0.75 words
- **Why important**: Controls how much text the AI generates

### Pipelines
- **What**: Pre-configured AI workflows for specific tasks
- **Examples**: text-generation, translation, sentiment-analysis
- **Why useful**: Makes complex AI tasks simple to use

## üîß Common Issues and Solutions

**Problem: "Model download is slow"**
```
Solution: Be patient! First download takes time.
The model is being saved to your computer for future use.
```

**Problem: "Out of memory error"**
```python
# Solution: Try a smaller model
model_id="gpt2"  # Much smaller than DialoGPT-medium
```

**Problem: "Response is weird or incomplete"**
```python
# Solution: Increase max_new_tokens
pipeline_kwargs={"max_new_tokens": 50}  # Generate more text
```

## üéØ Try These Experiments

### 1. Try Different Models
```python
# Fast and small
model_id="gpt2"

# Better for conversations  
model_id="microsoft/DialoGPT-medium"

# Very tiny but quick
model_id="distilgpt2"
```

### 2. Try Different Prompts
```python
prompt = "The weather today is"
prompt = "My favorite food is"
prompt = "In the future, computers will"
```

### 3. Generate More Text
```python
pipeline_kwargs={"max_new_tokens": 50}  # Longer responses
```

### 4. Add More Control
```python
pipeline_kwargs={
    "max_new_tokens": 30,
    "temperature": 0.7,      # Creativity level (0-1)
    "do_sample": True        # More random responses
}
```

## üåü Model Recommendations

### For Beginners:
- **`gpt2`**: Classic, reliable, small
- **`distilgpt2`**: Even smaller and faster

### For Better Quality:
- **`microsoft/DialoGPT-medium`**: Good for conversations
- **`EleutherAI/gpt-neo-1.3B`**: More sophisticated

### For Specific Tasks:
- **`facebook/bart-large-cnn`**: Summarization
- **`t5-small`**: Translation and tasks

## ü§î Understanding Model Sizes

- **Small models (< 500MB)**: Fast, basic responses
- **Medium models (500MB - 2GB)**: Good balance
- **Large models (> 2GB)**: Better quality, slower

## üåü What's Next?

Now that you understand local AI models, you're ready to learn about:
- **Prompt Templates** (Tutorial 3) - Creating reusable question formats
- **Chat Templates** (Tutorial 4) - Having longer conversations
- **Combining different AI models** - Using the best tool for each job

Congratulations! You now have your own personal AI assistant running on your computer! üéâ

## üîó Useful Resources

- **Hugging Face Model Hub**: https://huggingface.co/models
- **Model Documentation**: Look for model cards explaining each model
- **Community**: Hugging Face forums for help and discussion