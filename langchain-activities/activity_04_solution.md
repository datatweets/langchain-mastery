# üï∏Ô∏è Activity 04: Introduction to LangGraph - Master Solution Guide

## üìã Activity Overview

**Topic:** Building graph-based AI agents with LangGraph fundamentals  
**Duration:** 45-60 minutes  
**Difficulty:** Intermediate to Advanced  
**Prerequisites:** LangChain basics, understanding of workflow concepts

## üèÜ Complete Solution

### Step 1: Environment Setup

```python
# Install required libraries
!pip install --quiet langgraph==0.5.3 langchain-openai==0.3.16
```

### Step 2: Import Dependencies

```python
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
```

**Explanation:**
- `Annotated`: Adds metadata to type hints for enhanced functionality
- `TypedDict`: Creates strongly typed dictionaries for state management
- `StateGraph`: Core LangGraph class for building graph workflows
- `START`, `END`: Built-in nodes representing workflow entry and exit points
- `add_messages`: Function that appends messages to existing list rather than replacing
- `ChatOpenAI`: OpenAI's chat model integration for LangChain

### Step 3: Complete State and Graph Implementation

```python
# Create the state to capture the messages
class State(TypedDict):
    messages: Annotated[list, add_messages]

# Create the graph state
graph_builder = StateGraph(State)

# Define an OpenAI LLM
llm = ChatOpenAI(model="gpt-4o-mini")

# Takes the state, and appends the new messages to it
def llm_node(state: State):
    return {"messages": [llm.invoke(state["messages"])]}

# Create a node called "llm" that calls the llm_node function
graph_builder.add_node("llm", llm_node)

# Connect the "llm" node to the START and END of the graph
graph_builder.add_edge(START, "llm")
graph_builder.add_edge("llm", END)

# Compile the graph
graph = graph_builder.compile()
```

### Step 4: Testing Implementation

```python
# Visualize your graph
graph

# Test the graph with streaming
from course_helper_functions import pretty_print_messages

for chunk in graph.stream(
    {"messages": [{"role": "user", "content": "Tell me about Apple Inc."}]}
):
    pretty_print_messages(chunk)
```

## üß† Code Breakdown & Best Practices

### 1. State Architecture Deep Dive

```python
class State(TypedDict):
    messages: Annotated[list, add_messages]
```

**Advanced State Design Principles:**
- **Type Safety**: `TypedDict` provides compile-time type checking
- **Immutable Keys**: Dictionary structure is fixed and validated
- **Functional Updates**: `add_messages` ensures proper message appending
- **Memory Efficiency**: State modifications are handled optimally
- **Debugging Support**: Clear structure aids in troubleshooting

**Message Flow Mechanics:**
```python
# Initial state
{"messages": [{"role": "user", "content": "Hello"}]}

# After LLM node execution
{"messages": [
    {"role": "user", "content": "Hello"},
    {"role": "assistant", "content": "Hi! How can I help you?"}
]}

# Subsequent interactions append to the same list
{"messages": [
    {"role": "user", "content": "Hello"},
    {"role": "assistant", "content": "Hi! How can I help you?"},
    {"role": "user", "content": "What is AI?"},
    {"role": "assistant", "content": "AI stands for Artificial Intelligence..."}
]}
```

### 2. Graph Builder Pattern Analysis

```python
graph_builder = StateGraph(State)
```

**Builder Pattern Benefits:**
- **Incremental Construction**: Build complex graphs step by step
- **Validation**: Each step can be validated before compilation
- **Flexibility**: Easy to modify structure before compilation
- **Reusability**: Same builder can create multiple similar graphs
- **Error Prevention**: Catches structural issues before runtime

### 3. Node Function Architecture

```python
def llm_node(state: State):
    return {"messages": [llm.invoke(state["messages"])]}
```

**Node Function Design Principles:**

1. **Pure Function Design**: 
   - Input: Current state
   - Output: State updates
   - No side effects outside of return value

2. **State Consistency**:
   - Always return dictionary with expected keys
   - Maintain state structure integrity
   - Handle edge cases gracefully

3. **Message Context Management**:
   - LLM receives full conversation history
   - Context window automatically managed
   - Conversation continuity preserved

**Advanced Node Function Patterns:**

```python
# Pattern 1: Conditional Response
def conditional_llm_node(state: State):
    messages = state["messages"]
    last_message = messages[-1]["content"].lower()
    
    if "urgent" in last_message:
        # Use different model or prompt for urgent requests
        response = llm.invoke([
            {"role": "system", "content": "Respond with high priority and urgency."},
            *messages
        ])
    else:
        response = llm.invoke(messages)
    
    return {"messages": [response]}

# Pattern 2: Response Enhancement
def enhanced_llm_node(state: State):
    response = llm.invoke(state["messages"])
    
    # Add metadata or formatting
    enhanced_content = f"ü§ñ **AI Response**: {response.content}\n\n*Generated at: {datetime.now()}*"
    response.content = enhanced_content
    
    return {"messages": [response]}

# Pattern 3: Multi-step Processing
def processing_llm_node(state: State):
    # Step 1: Analyze user intent
    intent_analysis = llm.invoke([
        {"role": "system", "content": "Analyze the user's intent and respond with one word: QUESTION, REQUEST, or CONVERSATION"},
        state["messages"][-1]  # Only last message for intent
    ])
    
    # Step 2: Generate appropriate response based on intent
    if "QUESTION" in intent_analysis.content:
        system_prompt = "Provide a detailed, factual answer."
    elif "REQUEST" in intent_analysis.content:
        system_prompt = "Help the user accomplish their task."
    else:
        system_prompt = "Engage in friendly conversation."
    
    final_response = llm.invoke([
        {"role": "system", "content": system_prompt},
        *state["messages"]
    ])
    
    return {"messages": [final_response]}
```

### 4. Graph Construction and Flow Control

```python
# Node creation
graph_builder.add_node("llm", llm_node)

# Edge definitions
graph_builder.add_edge(START, "llm")
graph_builder.add_edge("llm", END)

# Graph compilation
graph = graph_builder.compile()
```

**Flow Control Mechanics:**
- **START Node**: Entry point for user input
- **Custom Nodes**: Processing components with state access
- **END Node**: Exit point returning final state
- **Compilation**: Converts builder instructions to executable graph

**Advanced Edge Patterns:**

```python
# Conditional edges (for future activities)
def route_condition(state):
    last_message = state["messages"][-1]["content"]
    if "question" in last_message.lower():
        return "answer_node"
    elif "help" in last_message.lower():
        return "help_node"
    else:
        return "chat_node"

# Multiple pathways
graph_builder.add_conditional_edges("classifier", route_condition, {
    "answer_node": "answer_processing",
    "help_node": "help_processing", 
    "chat_node": "casual_chat"
})
```

## üß™ Comprehensive Testing Suite

### Test Suite 1: Basic Functionality Validation

```python
def test_basic_graph_functionality():
    """Test core graph operations and state management"""
    
    # Test 1: Single message processing
    test_input = {"messages": [{"role": "user", "content": "Hello, how are you?"}]}
    
    result_chunks = list(graph.stream(test_input))
    
    # Validate structure
    assert len(result_chunks) > 0, "Graph should produce output chunks"
    
    final_state = result_chunks[-1]
    assert "messages" in final_state, "Final state should contain messages"
    assert len(final_state["messages"]) >= 2, "Should have user message and assistant response"
    
    # Validate message types
    messages = final_state["messages"]
    assert messages[0]["role"] == "user", "First message should be from user"
    assert messages[-1]["role"] == "assistant", "Last message should be from assistant"
    assert len(messages[-1]["content"]) > 0, "Assistant response should not be empty"
    
    print("‚úÖ Basic functionality test passed")
    
    # Test 2: Multi-turn conversation
    conversation_input = {"messages": [
        {"role": "user", "content": "What is Python?"},
        {"role": "assistant", "content": "Python is a programming language."},
        {"role": "user", "content": "What makes it popular?"}
    ]}
    
    result_chunks = list(graph.stream(conversation_input))
    final_state = result_chunks[-1]
    
    assert len(final_state["messages"]) == 4, "Should have 4 messages after processing"
    assert "programming" in final_state["messages"][-1]["content"].lower(), "Response should be contextually relevant"
    
    print("‚úÖ Multi-turn conversation test passed")

# Run basic tests
test_basic_graph_functionality()
```

### Test Suite 2: State Management and Persistence

```python
def test_state_management():
    """Test state persistence and message accumulation"""
    
    # Test message accumulation
    initial_messages = [
        {"role": "user", "content": "My name is Alice."},
    ]
    
    # First interaction
    result1 = list(graph.stream({"messages": initial_messages}))
    state_after_1 = result1[-1]["messages"]
    
    # Second interaction with accumulated state
    state_after_1.append({"role": "user", "content": "What is my name?"})
    result2 = list(graph.stream({"messages": state_after_1}))
    state_after_2 = result2[-1]["messages"]
    
    # Validate context retention
    final_response = state_after_2[-1]["content"].lower()
    assert "alice" in final_response, "LLM should remember the user's name from context"
    assert len(state_after_2) == 4, "Should have 4 messages: 2 user + 2 assistant"
    
    print("‚úÖ State persistence test passed")
    
    # Test state structure consistency
    for chunk in result2:
        assert isinstance(chunk, dict), "Each chunk should be a dictionary"
        assert "messages" in chunk, "Each chunk should have messages key"
        assert isinstance(chunk["messages"], list), "Messages should be a list"
    
    print("‚úÖ State structure consistency test passed")

# Run state management tests
test_state_management()
```

### Test Suite 3: Graph Structure and Compilation

```python
def test_graph_structure():
    """Test graph compilation and structure validation"""
    
    # Test graph creation from scratch
    test_graph_builder = StateGraph(State)
    
    # Add components
    test_graph_builder.add_node("test_llm", llm_node)
    test_graph_builder.add_edge(START, "test_llm")
    test_graph_builder.add_edge("test_llm", END)
    
    # Compile
    test_graph = test_graph_builder.compile()
    
    # Test execution
    test_result = list(test_graph.stream({
        "messages": [{"role": "user", "content": "Test message"}]
    }))
    
    assert len(test_result) > 0, "Test graph should produce output"
    assert "messages" in test_result[-1], "Output should contain messages"
    
    print("‚úÖ Graph structure test passed")
    
    # Test graph visualization (if supported)
    try:
        graph_repr = repr(test_graph)
        assert len(graph_repr) > 0, "Graph should have string representation"
        print("‚úÖ Graph visualization test passed")
    except Exception as e:
        print(f"‚ö†Ô∏è Graph visualization test skipped: {e}")

# Run graph structure tests  
test_graph_structure()
```

### Test Suite 4: Error Handling and Robustness

```python
def test_error_handling():
    """Test graph behavior with various error conditions"""
    
    # Test empty message handling
    try:
        empty_result = list(graph.stream({"messages": []}))
        # Should either handle gracefully or raise meaningful error
        print("‚úÖ Empty messages handled gracefully")
    except Exception as e:
        # Expected behavior - should be a meaningful error
        assert len(str(e)) > 0, "Error should be informative"
        print(f"‚úÖ Empty messages error handled: {type(e).__name__}")
    
    # Test malformed input
    try:
        malformed_result = list(graph.stream({"messages": "not a list"}))
        print("‚ö†Ô∏è Malformed input accepted (may need validation)")
    except Exception as e:
        print(f"‚úÖ Malformed input rejected appropriately: {type(e).__name__}")
    
    # Test very long conversation
    long_conversation = [
        {"role": "user" if i % 2 == 0 else "assistant", 
         "content": f"Message number {i}"}
        for i in range(20)  # 20 messages
    ]
    long_conversation.append({"role": "user", "content": "Summarize our conversation"})
    
    try:
        long_result = list(graph.stream({"messages": long_conversation}))
        assert len(long_result) > 0, "Should handle long conversations"
        print("‚úÖ Long conversation test passed")
    except Exception as e:
        print(f"‚ö†Ô∏è Long conversation issue: {e}")

# Run error handling tests
test_error_handling()
```

### Test Suite 5: Performance and Scalability

```python
import time

def test_performance():
    """Test graph performance characteristics"""
    
    # Test execution time
    test_messages = [
        {"role": "user", "content": "What is the capital of France?"}
    ]
    
    start_time = time.time()
    result = list(graph.stream({"messages": test_messages}))
    end_time = time.time()
    
    execution_time = end_time - start_time
    print(f"Single execution time: {execution_time:.2f} seconds")
    
    # Test multiple rapid executions
    start_time = time.time()
    for i in range(5):
        test_msg = {"messages": [{"role": "user", "content": f"Quick test {i}"}]}
        result = list(graph.stream(test_msg))
    end_time = time.time()
    
    batch_time = end_time - start_time
    avg_time = batch_time / 5
    print(f"Average execution time (5 tests): {avg_time:.2f} seconds")
    
    # Performance assertions
    assert execution_time < 30.0, "Single execution should complete within 30 seconds"
    assert avg_time < 10.0, "Average execution should be under 10 seconds"
    
    print("‚úÖ Performance tests passed")

# Run performance tests
test_performance()
```

## üéì Educational Insights

### Why LangGraph Represents a Paradigm Shift

1. **From Chains to Graphs**
   - **Traditional Chains**: Linear, rigid flow
   - **LangGraph**: Multi-path, conditional, flexible

2. **State-First Design**
   - **Traditional**: Stateless function calls
   - **LangGraph**: Persistent state throughout workflow

3. **Visual Debugging**
   - **Traditional**: Log-based debugging
   - **LangGraph**: Visual graph representation

4. **Compositional Architecture**
   - **Traditional**: Monolithic agent design
   - **LangGraph**: Modular, reusable components

### Graph Theory Applied to AI Systems

**Node Types in AI Workflows:**
- **Processing Nodes**: LLM calls, tool executions
- **Decision Nodes**: Routing, conditional logic
- **State Nodes**: Data transformation, validation
- **Human Nodes**: Human-in-the-loop interactions

**Edge Types in AI Workflows:**
- **Sequential Edges**: Linear progression
- **Conditional Edges**: Dynamic routing based on state
- **Parallel Edges**: Concurrent execution paths
- **Loop Edges**: Iterative processing cycles

### Advanced State Management Patterns

```python
# Pattern 1: Multi-domain state
class AdvancedState(TypedDict):
    messages: Annotated[list, add_messages]
    user_profile: dict
    context: dict
    metadata: dict

# Pattern 2: State validation
def validate_state(state: State) -> State:
    """Validate and sanitize state before processing"""
    if not state.get("messages"):
        state["messages"] = []
    
    # Ensure message format consistency
    for msg in state["messages"]:
        if "role" not in msg or "content" not in msg:
            raise ValueError(f"Invalid message format: {msg}")
    
    return state

# Pattern 3: State transformation
def transform_state(state: State) -> State:
    """Transform state for specific processing needs"""
    # Example: Limit conversation history
    if len(state["messages"]) > 20:
        # Keep first message (system) and last 19 messages
        state["messages"] = [state["messages"][0]] + state["messages"][-19:]
    
    return state
```

## üîß Advanced Implementation Variations

### Variation 1: Multi-Model Graph

```python
from langchain_openai import ChatOpenAI

# Different models for different purposes
class MultiModelState(TypedDict):
    messages: Annotated[list, add_messages]
    model_choice: str

fast_llm = ChatOpenAI(model="gpt-3.5-turbo")
smart_llm = ChatOpenAI(model="gpt-4o")
creative_llm = ChatOpenAI(model="gpt-4o", temperature=0.8)

def adaptive_llm_node(state: MultiModelState):
    """Choose LLM based on request type"""
    last_message = state["messages"][-1]["content"].lower()
    
    if any(word in last_message for word in ["quick", "simple", "fast"]):
        llm = fast_llm
        model_used = "fast"
    elif any(word in last_message for word in ["complex", "analyze", "detailed"]):
        llm = smart_llm  
        model_used = "smart"
    elif any(word in last_message for word in ["creative", "story", "poem"]):
        llm = creative_llm
        model_used = "creative"
    else:
        llm = smart_llm
        model_used = "default"
    
    response = llm.invoke(state["messages"])
    return {
        "messages": [response],
        "model_choice": model_used
    }

# Build multi-model graph
multi_model_builder = StateGraph(MultiModelState)
multi_model_builder.add_node("adaptive_llm", adaptive_llm_node)
multi_model_builder.add_edge(START, "adaptive_llm")
multi_model_builder.add_edge("adaptive_llm", END)
multi_model_graph = multi_model_builder.compile()
```

### Variation 2: Logging and Monitoring Graph

```python
import logging
from datetime import datetime

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MonitoredState(TypedDict):
    messages: Annotated[list, add_messages]
    execution_log: list

def logged_llm_node(state: MonitoredState):
    """LLM node with comprehensive logging"""
    start_time = datetime.now()
    
    try:
        # Log incoming request
        logger.info(f"Processing request at {start_time}")
        logger.info(f"Message count: {len(state['messages'])}")
        
        # Process request
        response = llm.invoke(state["messages"])
        
        # Log success
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        logger.info(f"Request processed in {duration:.2f} seconds")
        
        # Update execution log
        log_entry = {
            "timestamp": start_time.isoformat(),
            "duration_seconds": duration,
            "message_count": len(state["messages"]),
            "status": "success",
            "response_length": len(response.content)
        }
        
        execution_log = state.get("execution_log", [])
        execution_log.append(log_entry)
        
        return {
            "messages": [response],
            "execution_log": execution_log
        }
        
    except Exception as e:
        # Log error
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        logger.error(f"Request failed after {duration:.2f} seconds: {e}")
        
        # Update execution log with error
        log_entry = {
            "timestamp": start_time.isoformat(),
            "duration_seconds": duration,
            "message_count": len(state["messages"]),
            "status": "error",
            "error": str(e)
        }
        
        execution_log = state.get("execution_log", [])
        execution_log.append(log_entry)
        
        # Return error message
        from langchain_core.messages import AIMessage
        error_response = AIMessage(content=f"I encountered an error: {str(e)}")
        
        return {
            "messages": [error_response],
            "execution_log": execution_log
        }

# Build monitored graph
monitored_builder = StateGraph(MonitoredState)
monitored_builder.add_node("logged_llm", logged_llm_node)
monitored_builder.add_edge(START, "logged_llm")
monitored_builder.add_edge("logged_llm", END)
monitored_graph = monitored_builder.compile()
```

### Variation 3: Human-in-the-Loop Graph

```python
class HumanState(TypedDict):
    messages: Annotated[list, add_messages]
    requires_human_review: bool
    human_approved: bool

def content_filter_node(state: HumanState):
    """Check if content requires human review"""
    last_message = state["messages"][-1]["content"]
    
    # Simple content filtering logic
    sensitive_keywords = ["legal", "medical", "financial advice", "investment"]
    requires_review = any(keyword in last_message.lower() for keyword in sensitive_keywords)
    
    return {
        "messages": [],  # No new messages added
        "requires_human_review": requires_review,
        "human_approved": False
    }

def human_review_node(state: HumanState):
    """Simulate human review process"""
    print("üö® HUMAN REVIEW REQUIRED")
    print(f"User message: {state['messages'][-1]['content']}")
    
    # In a real implementation, this would pause for human input
    # For demo, we'll auto-approve
    approval = input("Approve this request? (y/n): ").lower().startswith('y')
    
    return {
        "messages": [],
        "human_approved": approval
    }

def conditional_llm_node(state: HumanState):
    """LLM node that only processes approved content"""
    if state.get("requires_human_review", False) and not state.get("human_approved", False):
        from langchain_core.messages import AIMessage
        rejection_msg = AIMessage(content="I cannot process this request without human approval.")
        return {"messages": [rejection_msg]}
    
    # Normal LLM processing
    response = llm.invoke(state["messages"])
    return {"messages": [response]}

# Build human-in-the-loop graph with conditional routing
human_builder = StateGraph(HumanState)
human_builder.add_node("filter", content_filter_node)
human_builder.add_node("human_review", human_review_node)
human_builder.add_node("llm", conditional_llm_node)

# Add edges
human_builder.add_edge(START, "filter")

# Conditional edges based on review requirement
def should_review(state: HumanState):
    return "human_review" if state.get("requires_human_review", False) else "llm"

human_builder.add_conditional_edges("filter", should_review, {
    "human_review": "human_review",
    "llm": "llm"
})

human_builder.add_edge("human_review", "llm")
human_builder.add_edge("llm", END)

human_graph = human_builder.compile()
```

## üìä Performance Optimization Strategies

### 1. State Size Management

```python
def optimize_state_size(state: State) -> State:
    """Keep state size manageable"""
    MAX_MESSAGES = 50
    
    if len(state["messages"]) > MAX_MESSAGES:
        # Keep system message and recent messages
        system_msgs = [msg for msg in state["messages"] if msg.get("role") == "system"]
        recent_msgs = state["messages"][-(MAX_MESSAGES-len(system_msgs)):]
        state["messages"] = system_msgs + recent_msgs
    
    return state

def memory_efficient_llm_node(state: State):
    """LLM node with state optimization"""
    optimized_state = optimize_state_size(state)
    response = llm.invoke(optimized_state["messages"])
    return {"messages": [response]}
```

### 2. Parallel Processing Preparation

```python
# Structure for future parallel node execution
class ParallelState(TypedDict):
    messages: Annotated[list, add_messages]
    parallel_results: dict

def prepare_parallel_node(state: ParallelState):
    """Prepare state for parallel processing"""
    # This would be used with parallel edges in advanced graphs
    return {
        "messages": [],
        "parallel_results": {}
    }
```

### 3. Caching Integration

```python
from functools import lru_cache
import hashlib

def hash_messages(messages: list) -> str:
    """Create hash of message content for caching"""
    content = "".join(msg["content"] for msg in messages)
    return hashlib.md5(content.encode()).hexdigest()

# Simple in-memory cache
response_cache = {}

def cached_llm_node(state: State):
    """LLM node with response caching"""
    messages_hash = hash_messages(state["messages"])
    
    if messages_hash in response_cache:
        print("üéØ Cache hit - returning cached response")
        return {"messages": [response_cache[messages_hash]]}
    
    response = llm.invoke(state["messages"])
    response_cache[messages_hash] = response
    
    return {"messages": [response]}
```

## üìù Assessment Rubric

### Functionality (40 points)
- **State creation:** Proper TypedDict and Annotated usage (10 pts)
- **Graph building:** Correct StateGraph initialization and node addition (10 pts)
- **Flow control:** Proper edge connections and compilation (10 pts)
- **Execution:** Successfully streams and processes messages (10 pts)

### Code Quality (35 points)
- **Type safety:** Proper use of type hints and annotations (10 pts)
- **Architecture:** Clean separation of concerns and modularity (15 pts)
- **Error handling:** Robust error management (10 pts)

### Understanding (25 points)
- **Graph concepts:** Explains nodes, edges, and state management (15 pts)
- **Workflow design:** Understands how graphs improve agent architecture (10 pts)

### Total: 100 points

**Grading Scale:**
- 90-100: Excellent grasp of graph-based agent architecture
- 80-89: Good implementation with minor conceptual gaps
- 70-79: Basic functionality, needs deeper understanding
- Below 70: Requires additional study of graph concepts

## üöÄ Real-World Applications

### Enterprise Workflow Automation

```python
# Example: Customer service workflow
class CustomerServiceState(TypedDict):
    messages: Annotated[list, add_messages]
    customer_info: dict
    ticket_priority: str
    resolution_path: str

def classify_request(state: CustomerServiceState):
    """Classify customer request type"""
    last_message = state["messages"][-1]["content"]
    
    if any(word in last_message.lower() for word in ["urgent", "emergency", "critical"]):
        priority = "high"
    elif any(word in last_message.lower() for word in ["billing", "payment", "refund"]):
        priority = "medium"  
    else:
        priority = "low"
    
    return {"ticket_priority": priority}

def route_to_specialist(state: CustomerServiceState):
    """Route to appropriate handling path"""
    priority = state.get("ticket_priority", "low")
    
    if priority == "high":
        return "escalation_node"
    elif priority == "medium":
        return "billing_specialist"
    else:
        return "general_support"
```

### Content Creation Pipeline

```python
class ContentState(TypedDict):
    messages: Annotated[list, add_messages]
    content_type: str
    draft_content: str
    review_status: str
    final_content: str

def content_planner(state: ContentState):
    """Plan content structure"""
    request = state["messages"][-1]["content"]
    
    planning_prompt = f"""
    Create a content outline for: {request}
    Provide a structured plan with main sections and key points.
    """
    
    plan = llm.invoke([{"role": "user", "content": planning_prompt}])
    return {"messages": [plan]}

def content_writer(state: ContentState):
    """Generate content based on plan"""
    plan = state["messages"][-1]["content"]
    
    writing_prompt = f"""
    Based on this content plan:
    {plan}
    
    Write the complete content with engaging style and clear structure.
    """
    
    draft = llm.invoke([{"role": "user", "content": writing_prompt}])
    return {
        "messages": [draft],
        "draft_content": draft.content
    }
```

## üí° Pro Tips for Instructors

1. **Visual Learning**: Always show the graph visualization to help students understand structure
2. **State First**: Emphasize state design before building nodes and edges
3. **Incremental Complexity**: Start with linear flows before introducing conditionals
4. **Real Examples**: Use concrete business scenarios students can relate to
5. **Debug Together**: Walk through state changes step by step during execution
6. **Architecture Benefits**: Compare with traditional approaches to highlight advantages

## üèÅ Conclusion

This exercise establishes the foundation for sophisticated graph-based AI agent systems. Students learn:

- **Graph Thinking**: How to decompose workflows into nodes and edges
- **State Management**: Persistent data flow across complex processes  
- **Visual Architecture**: Understanding systems through graph visualization
- **Modular Design**: Building reusable, composable agent components
- **Scalable Patterns**: Foundations for enterprise-grade agent systems

The graph-based approach opens up possibilities for:
- Multi-agent collaboration
- Human-in-the-loop workflows
- Conditional routing and decision making
- Parallel processing and optimization
- Complex state management and persistence

Students are now ready to build sophisticated, production-ready AI agent systems! üéìüï∏Ô∏è